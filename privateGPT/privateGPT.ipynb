{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanland/colab-notebooks/blob/main/privateGPT/privateGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg8LXAe8mt-G"
      },
      "source": [
        "## privateGPT\n",
        "\n",
        "https://github.com/imartinez/privateGPT\n",
        "\n",
        "\n",
        "More details can be founded in [![GitHub](https://img.shields.io/github/stars/imartinez/privateGPT?style=social)](https://github.com/imartinez/privateGPT/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijIJmBFLebe3",
        "outputId": "d0e7df60-b0fd-4c24-d609-1d485eb79435",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4, 15360 MiB, 15101 MiB\n"
          ]
        }
      ],
      "source": [
        "### make sure that CUDA is available in Edit -> Nootbook settings -> GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn_LkaeUmFJQ"
      },
      "source": [
        "## Installnation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4XypmLmfJNw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title python 3.8 | 不执行则使用系统版本 python\n",
        "!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.8 2  \n",
        "!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.9 1  \n",
        "!python --version  \n",
        "!apt-get update\n",
        "!apt install software-properties-common\n",
        "!sudo dpkg --remove --force-remove-reinstreq python3-pip python3-setuptools python3-wheel\n",
        "!apt-get install python3-pip\n",
        "\n",
        "print('Git clone project and install requirements...')\n",
        "!git clone https://github.com/VideoCrafter/VideoCrafter &> /dev/null\n",
        "%cd VideoCrafter \n",
        "!export PYTHONPATH=/content/VideoCrafter:$PYTHONPATH \n",
        "\n",
        "!python3.8 -m pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!apt update\n",
        "!apt install ffmpeg &> /dev/null  \n",
        "!python3.8 -m pip install pytorch-lightning==1.8.3 omegaconf==2.1.1 einops==0.3.0 transformers==4.25.1\n",
        "!python3.8 -m pip install opencv-python==4.1.2.30 imageio==2.9.0 imageio-ffmpeg==0.4.2\n",
        "!python3.8 -m pip install av moviepy\n",
        "!python3.8 -m pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_y8F_z_-hd9w",
        "outputId": "ea184b80-3fab-4bd2-e9b4-af3e468ba401",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Failed to call git rev-parse --git-dir: exit status 128 \n",
            "Git LFS initialized.\n",
            "Cloning into 'privateGPT'...\n",
            "remote: Enumerating objects: 286, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 286 (delta 0), reused 2 (delta 0), pack-reused 282\u001b[K\n",
            "Receiving objects: 100% (286/286), 103.03 KiB | 7.92 MiB/s, done.\n",
            "Resolving deltas: 100% (143/143), done.\n",
            "/content/privateGPT\n"
          ]
        }
      ],
      "source": [
        "#@title git clone\n",
        "!git lfs install\n",
        "!git clone https://github.com/imartinez/privateGPT.git\n",
        "%cd privateGPT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title checkout\n",
        "#!git checkout main-local\n",
        "!git pull"
      ],
      "metadata": {
        "id": "7udHDXrw4Aq1",
        "outputId": "87d272e4-f0d1-4622-cfcc-7d8242c54f9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title install dependencies\n",
        "\n",
        "# 安装依赖\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Install dependencies\n",
        "%cd /content/privateGPT\n",
        "!pip3 install -r requirements.txt\n",
        "\n",
        "%cd /content\n",
        "!rm -rf langchain\n",
        "!rm -rf llama-cpp-python\n",
        "!git clone https://github.com/hwchase17/langchain\n",
        "!git clone https://github.com/abetlen/llama-cpp-python\n",
        "%cd llama-cpp-python/vendor\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "\n",
        "%cd /content\n",
        "\n",
        "!pip3 uninstall llama-cpp-python -y\n",
        "!pip3 install scikit-build\n",
        "%cd /content/llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 python3 ./setup.py install\n",
        "\n",
        "%cd /content/langchain\n",
        "!pip3 uninstall langchain -y\n",
        "!pip3 install -e ."
      ],
      "metadata": {
        "id": "yDxaRuBP4KoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Koala 7B GGML Q4\n",
        "!mkdir /content/privateGPT/models/\n",
        "%cd /content/privateGPT/models/\n",
        "#!wget https://huggingface.co/TheBloke/koala-7B-GGML/resolve/main/koala-7B.ggmlv3.q4_0.bin\n",
        "!wget https://huggingface.co/gorborukov/koala-7b-ggml-q4_0/resolve/main/koala-7b-ggml-q4_0.bin"
      ],
      "metadata": {
        "id": "qypgc1qrvfX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls\n",
        "%ls models/"
      ],
      "metadata": {
        "id": "B2q6gGJv1ClM",
        "outputId": "5ee8e4da-3cdc-41b7-e1a9-b9cef7ac088c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "constants.py  \u001b[0m\u001b[01;32mingest.py\u001b[0m*  \u001b[01;34mmodels\u001b[0m/         \u001b[01;34m__pycache__\u001b[0m/  requirements.txt\n",
            "\u001b[01;34mdb\u001b[0m/           LICENSE     \u001b[01;32mprivateGPT.py\u001b[0m*  README.md     \u001b[01;34msource_documents\u001b[0m/\n",
            "koala-7b-ggml-q4_0.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Modify .env\n",
        "!cp /content/privateGPT/example.env /content/privateGPT/.env \n",
        "!sed -i 's/n_gpu_layers=10/n_gpu_layers=500/' /content/privateGPT/.env \n",
        "!sed -i 's/MODEL_TYPE=GPT4All/MODEL_TYPE=LlamaCpp/' /content/privateGPT/.env \n",
        "!sed -i 's/ggml-gpt4all-j-v1.3-groovy.bin/koala-7b-ggml-q4_0.bin/' /content/privateGPT/.env "
      ],
      "metadata": {
        "id": "jxrPTXE0sgcZ",
        "outputId": "65bd1299-7445-49b9-a2a4-b9e833e33b06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '/content/privateGPT/example.env': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title These were required before (for some filetypes), idk now\n",
        "!apt install pandoc\n",
        "!pip3 install unstructured"
      ],
      "metadata": {
        "id": "Gi4kGu_4zsjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Ingest sources\n",
        "%cd  /content/privateGPT/\n",
        "!python3 ingest.py"
      ],
      "metadata": {
        "id": "UaNIO6crz2x3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/privateGPT\n",
        "%ls source_documents\n",
        "!curl https://ttx-download.oss-cn-hangzhou.aliyuncs.com/temp/%E5%86%B7%E7%9F%A5%E8%AF%86.txt -o source_documents/lengzhishi.txt"
      ],
      "metadata": {
        "id": "U3pVnaa2tIQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run the chatbot\n",
        "%ls models\n",
        "%ls models/koala-7B.ggml.q4_0.bin\n",
        "%cd /content/privateGPT\n",
        "!python3 privateGPT.py"
      ],
      "metadata": {
        "id": "mqa9t41jt1ZK",
        "outputId": "78dad30e-2595-43a4-9a5d-ae3890d0dadd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "koala-7b-ggml-q4_0.bin\n",
            "ls: cannot access 'models/koala-7B.ggml.q4_0.bin': No such file or directory\n",
            "/content/privateGPT\n",
            "Using embedded DuckDB with persistence: data will be stored in: db\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/privateGPT/privateGPT.py\", line 76, in <module>\n",
            "    main()\n",
            "  File \"/content/privateGPT/privateGPT.py\", line 34, in main\n",
            "    llm = LlamaCpp(model_path=model_path, n_ctx=model_n_ctx, callbacks=callbacks, verbose=False)\n",
            "  File \"pydantic/main.py\", line 341, in pydantic.main.BaseModel.__init__\n",
            "pydantic.error_wrappers.ValidationError: 1 validation error for LlamaCpp\n",
            "__root__\n",
            "  Could not load Llama model from path: models/koala-7B.ggml.q4_0.bin. Received error Model path does not exist: models/koala-7B.ggml.q4_0.bin (type=value_error)\n",
            "Exception ignored in: <function Llama.__del__ at 0x7f6a10dfb2e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/llama_cpp_python-0.1.58-py3.10-linux-x86_64.egg/llama_cpp/llama.py\", line 1333, in __del__\n",
            "    if self.ctx is not None:\n",
            "AttributeError: 'Llama' object has no attribute 'ctx'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear database\n",
        "!rm -rf /content/privateGPT/db"
      ],
      "metadata": {
        "id": "d2hePzFE0GK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Zip the database if will reuse without recreating vectors\n",
        "!zip -r /content/db.zip /content/privateGPT/db"
      ],
      "metadata": {
        "id": "In4loHLa0IWy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}