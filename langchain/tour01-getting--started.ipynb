{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanland/colab-notebooks/blob/main/langchain/tour01-getting--started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg8LXAe8mt-G"
      },
      "source": [
        "# 快速入门指南\n",
        "本教程让您快速了解如何使用 LangChain 构建端到端语言模型应用程序。\n",
        "\n",
        "安装\n",
        "首先，使用以下命令安装 LangChain："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ijIJmBFLebe3",
        "outputId": "1b80bd29-45f6-45d7-be72-468af4147677",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.198-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.4)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.7 (from langchain)\n",
            "  Downloading langchainplus_sdk-0.0.9-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.8 langchain-0.0.198 langchainplus-sdk-0.0.9 marshmallow-3.19.0 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install google-search-results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn_LkaeUmFJQ"
      },
      "source": [
        "##环境设置\n",
        "使用 LangChain 通常需要与一个或多个模型提供者、数据存储、api 等集成。\n",
        "\n",
        "对于这个例子，我们将使用 OpenAI 的 API，所以我们首先需要安装他们的 SDK："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "l4XypmLmfJNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12bc2492-6674-474a-f631-39978b9654b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Collecting aiohttp (from openai)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->openai)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->openai)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->openai)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->openai)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->openai)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.8 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "然后我们需要在终端中设置环境变量。"
      ],
      "metadata": {
        "id": "WBoWlgRsHiJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export OPENAI_API_KEY=\"...\""
      ],
      "metadata": {
        "id": "wpx9MUD8Hm-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "或者，您可以从 Jupyter notebook（或 Python 脚本）中执行此操作："
      ],
      "metadata": {
        "id": "x5fT6vg1ICIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"...\""
      ],
      "metadata": {
        "id": "pqJwYvOBICpc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果要动态设置 API 密钥，可以在启动 OpenAI 类时使用 openai_api_key 参数——例如，每个用户的 API 密钥。"
      ],
      "metadata": {
        "id": "SXHunVn9IDEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(openai_api_key=\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "KO4xBAJuIDfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 构建语言模型应用程序：LLM\n",
        "现在我们已经安装了 LangChain 并设置了我们的环境，我们可以开始构建我们的语言模型应用程序了。\n",
        "\n",
        "LangChain 提供了很多可以用来构建语言模型应用的模块。模块可以组合起来创建更复杂的应用程序，或者单独用于简单的应用程序。\n",
        "\n",
        "LangChain 最基本的构建块是在某些输入上调用 LLM。让我们通过一个简单的例子来说明如何做到这一点。为此，假设我们正在构建一项服务，该服务会根据公司的产品生成公司名称。\n",
        "\n",
        "为此，我们首先需要导入 LLM 包装器。"
      ],
      "metadata": {
        "id": "eX3B8WgJIagX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "vAY3ye9PILUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "然后我们可以用任何参数初始化包装器。在此示例中，我们可能希望输出更加随机，因此我们将使用高温对其进行初始化。"
      ],
      "metadata": {
        "id": "t9MsPv0cImBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.9)"
      ],
      "metadata": {
        "id": "YE-mpF5_I1VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们现在可以根据一些输入调用它！"
      ],
      "metadata": {
        "id": "C5WOWbKcI044"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"帮我给一家做 AI 服务的科技公司起一个中文名和英文名\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgchcTSJI0fT",
        "outputId": "b5b597bf-1ae2-4360-9775-a18574faee38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "中文名：智能联盟\n",
            "英文名：Intelligent Alliance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"帮我给一家做 AI 服务的科技公司起一个中文名和英文名\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "outputId": "4ebb3623-9dd3-4498-c26d-93f76dc566ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhY4BX7nJwW2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "中文名：智博科技\n",
            "英文名：InteBridge Technology\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 提示模板：管理 LLM 的提示\n",
        "获得LLM是重要的第一步，但这仅仅是个开始。通常，当您在应用程序中使用 LLM 时，您不会将用户输入直接发送到 LLM。相反，您可能正在接受用户输入并构建提示，然后将其发送给 LLM。\n",
        "\n",
        "例如，在前面的示例中，我们传入的文本被硬编码为要求一家公司的名称。在这个假想的服务中，我们想要做的是只接受描述公司所做的用户输入，然后用该信息格式化提示。\n",
        "\n",
        "使用 LangChain 很容易做到这一点！\n",
        "\n",
        "首先让我们定义提示模板："
      ],
      "metadata": {
        "id": "Zwy7fX7uI0Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"帮我给一家做 {product} 服务的科技公司起一个中文名和英文名\",\n",
        ")"
      ],
      "metadata": {
        "id": "r5_C6CjZI0Sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在让我们看看这是如何工作的！我们可以调用该.format方法对其进行格式化。"
      ],
      "metadata": {
        "id": "MBIXD9e0I0ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format(product=\"相册\"))"
      ],
      "metadata": {
        "id": "woaMouCiI0Gq",
        "outputId": "eb64dd65-a570-43c8-ebdb-fd520dc56002",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "帮我给一家做 相册 服务的科技公司起一个中文名和英文名\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains：在多步骤工作流程中结合 LLM 和提示\n",
        "到目前为止，我们一直在单独使用 PromptTemplate 和 LLM 原语。但是，当然，真正的应用程序不仅仅是一个原语，而是它们的组合。\n",
        "\n",
        "LangChain 中的一条链由链接组成，链接可以是像 LLM 这样的原始链，也可以是其他链。\n",
        "\n",
        "最核心的链类型是 LLMChain，它由 PromptTemplate 和 LLM 组成。\n",
        "\n",
        "扩展前面的示例，我们可以构造一个 LLMChain，它接受用户输入，使用 PromptTemplate 对其进行格式化，然后将格式化的响应传递给 LLM。"
      ],
      "metadata": {
        "id": "RETA5ff3I0A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"帮我给一家做 {product} 服务的科技公司起一个中文名和英文名\",\n",
        ")"
      ],
      "metadata": {
        "id": "5vaa9NjLIz6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们现在可以创建一个非常简单的链，它将接受用户输入，用它格式化提示，然后将它发送到 LLM："
      ],
      "metadata": {
        "id": "x0jCSH-2Iz0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "JY1j341DIzrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "现在我们可以仅指定产品来运行该链！"
      ],
      "metadata": {
        "id": "l2tmnIJ_Izl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"电子音乐\")"
      ],
      "metadata": {
        "id": "rz4o0Xl-Izfz",
        "outputId": "bd0e3987-8381-4095-fad8-8cf4f60c5ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n中文名：节拍音乐 \\n英文名：Beats Music'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 代理：基于用户输入的动态调用链\n",
        "到目前为止，我们所看到的链条以预定的顺序运行。\n",
        "\n",
        "代理人不再这样做：他们使用 LLM 来确定采取哪些行动以及采取何种顺序。动作可以是使用工具并观察其输出，也可以是返回给用户。\n",
        "\n",
        "如果使用得当，代理可以非常强大。在本教程中，我们将向您展示如何通过最简单、最高级别的 API 轻松使用代理。\n",
        "\n",
        "为了加载代理，您应该了解以下概念：\n",
        "\n",
        "- 工具：执行特定任务的功能。这可以是：Google 搜索、数据库查找、Python REPL、其他链。工具的接口目前是一个函数，期望将字符串作为输入，将字符串作为输出。\n",
        "- LLM：为代理提供支持的语言模型。\n",
        "- 代理：要使用的代理。这应该是一个引用支持代理类的字符串。由于本笔记本侧重于最简单、最高级别的 API，因此仅涵盖使用标准支持的代理。如果您想实施自定义代理，请参阅自定义代理的文档（即将推出）。\n",
        "\n",
        "**代理**：有关受支持代理及其规范的列表，请参见此处。\n",
        "\n",
        "**工具**：有关预定义工具及其规格的列表，请参见此处。\n",
        "\n",
        "对于此示例，您还需要安装 SerpAPI Python 包。"
      ],
      "metadata": {
        "id": "PvsCOIAdLD9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results"
      ],
      "metadata": {
        "id": "UxTdL-_5LD3s",
        "outputId": "7b90040a-f9bf-4d39-82d5-7aaac571c27f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32002 sha256=8914491a9bc2c1d16f26fc5253d181d6030952be1be807976bb0522b1b600306\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "并设置相应的环境变量。"
      ],
      "metadata": {
        "id": "NGo33dm0LDx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"...\""
      ],
      "metadata": {
        "id": "jyTD0fS6LDr-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在我们可以开始了！"
      ],
      "metadata": {
        "id": "yPRIJtoOLDlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# First, let's load the language model we're going to use to control the agent.\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now let's test it out!\n",
        "agent.run(\"上海昨天的高温是多少度？ 这个数字的 .023 次方是多少\")"
      ],
      "metadata": {
        "id": "qmXHpyHtLDgE",
        "outputId": "8ad513ff-f69d-4108-9986-a4be5567c030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m I need to find out the temperature in Shanghai yesterday and then calculate the .023 power of that number\n",
            "Action: Search\n",
            "Action Input: \"Shanghai temperature yesterday\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mhttps://www.timeanddate.com/weather/china/shanghai/historic\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I can use the calculator to calculate the .023 power\n",
            "Action: Calculator\n",
            "Action Input: \"Temperature in Shanghai yesterday (34°C) ^ .023\"\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.084486162715683\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The .023 power of the temperature in Shanghai yesterday (34°C) is 1.084486162715683.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The .023 power of the temperature in Shanghai yesterday (34°C) is 1.084486162715683.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 内存：将状态添加到链和代理中\n",
        "到目前为止，我们所经历的所有链和代理都是无状态的。但通常，您可能希望链或代理具有某种“记忆”概念，以便它可以记住有关其先前交互的信息。最清晰和简单的例子是在设计聊天机器人时——您希望它记住以前的消息，以便它可以使用来自该消息的上下文来进行更好的对话。这将是一种“短期记忆”。在更复杂的方面，你可以想象一个链/代理随着时间的推移记住关键信息——这将是一种“长期记忆”。有关后者的更具体想法，请参阅这篇很棒的论文。\n",
        "\n",
        "LangChain 专门为此提供了几个专门创建的链。本笔记本介绍了如何使用这些链中的一个 (the ConversationChain) 和两种不同类型的内存。\n",
        "\n",
        "默认情况下，ConversationChain有一种简单类型的内存，可以记住所有以前的输入/输出并将它们添加到传递的上下文中。让我们来看看使用这个链（设置verbose=True以便我们可以看到提示）。"
      ],
      "metadata": {
        "id": "9tOrbk56LDal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "output = conversation.predict(input=\"你好\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "TXjGO9mrLDUb",
        "outputId": "040e9885-df2a-49a1-d795-e3b73568e2f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: 你好\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " 你好！很高兴见到你！我是一个智能AI，我可以回答你的问题，或者我们可以聊聊天？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = conversation.predict(input=\"我正在跟 AI 聊天呢。\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "geGbIHxcNnmJ",
        "outputId": "6af50c4f-ab8d-4ffb-ca37-350dd6008ad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 你好\n",
            "AI:  你好！很高兴见到你！我是一个智能AI，我可以回答你的问题，或者我们可以聊聊天？\n",
            "Human: 我正在跟 AI 聊天呢。\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " 哦，真的吗？这太棒了！我很高兴能和你聊天！有什么可以帮助你的吗？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 构建语言模型应用程序：聊天模型\n",
        "同样，您可以使用聊天模型而不是 LLM。聊天模型是语言模型的变体。虽然聊天模型在底层使用语言模型，但它们公开的接口有点不同：它们公开的不是“文本输入、文本输出”API，而是“聊天消息”作为输入和输出的接口。\n",
        "\n",
        "聊天模型 API 相当新，所以我们仍在寻找正确的抽象。\n",
        "\n",
        "## 从聊天模型中获取消息完成\n",
        "您可以通过将一条或多条消息传递给聊天模型来获得聊天完成。响应将是一条消息。LangChain 目前支持的消息类型有AIMessage, HumanMessage, SystemMessage, 和ChatMessage–ChatMessage接受任意角色参数。大多数时候，您只会处理HumanMessage、AIMessage和SystemMessage。\n"
      ],
      "metadata": {
        "id": "-vRLWNJ3LDN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "CdVLwvN8LDIE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "您可以通过传递一条消息来完成。"
      ],
      "metadata": {
        "id": "lNdZV9ZdLDB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat([HumanMessage(content=\"Translate this sentence from English to Chinese. I love programming.\")])\n",
        "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
      ],
      "metadata": {
        "id": "v5XGFwTsLC8E",
        "outputId": "b851d93a-6f9e-4714-925a-58882feec5c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(lc_kwargs={'content': '我喜欢编程。'}, content='我喜欢编程。', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "您还可以为 OpenAI 的 gpt-3.5-turbo 和 gpt-4 模型传入多条消息。"
      ],
      "metadata": {
        "id": "4OKkvs5PLC2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates Chinese to English.\"),\n",
        "    HumanMessage(content=\"我喜欢编程。\")\n",
        "]\n",
        "chat(messages)\n",
        "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
      ],
      "metadata": {
        "id": "ul8NnIANLCvu",
        "outputId": "39225ebf-b567-481f-9a24-1cb9492dc005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(lc_kwargs={'content': '\"I like programming.\"'}, content='\"I like programming.\"', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "您可以更进一步，使用 为多组消息生成补全generate。这将返回LLMResult带有附加message参数的 ："
      ],
      "metadata": {
        "id": "Zo0-E7BvLCn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_messages = [\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to Chinese.\"),\n",
        "        HumanMessage(content=\"I love programming.\")\n",
        "    ],\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to Chinese.\"),\n",
        "        HumanMessage(content=\"I love artificial intelligence.\")\n",
        "    ],\n",
        "]\n",
        "result = chat.generate(batch_messages)\n",
        "result"
      ],
      "metadata": {
        "id": "Kvq3FcXYLCdH",
        "outputId": "5a36debe-32b6-48f5-f14e-83213ef3ca8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[ChatGeneration(lc_kwargs={'message': AIMessage(lc_kwargs={'content': '我喜欢编程。'}, content='我喜欢编程。', additional_kwargs={}, example=False)}, text='我喜欢编程。', generation_info=None, message=AIMessage(lc_kwargs={'content': '我喜欢编程。'}, content='我喜欢编程。', additional_kwargs={}, example=False))], [ChatGeneration(lc_kwargs={'message': AIMessage(lc_kwargs={'content': '我喜欢人工智能。'}, content='我喜欢人工智能。', additional_kwargs={}, example=False)}, text='我喜欢人工智能。', generation_info=None, message=AIMessage(lc_kwargs={'content': '我喜欢人工智能。'}, content='我喜欢人工智能。', additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 19, 'total_tokens': 76}, 'model_name': 'gpt-3.5-turbo'}, run=RunInfo(run_id=UUID('7212ce06-5e1f-43d7-8e12-498d977e1e3a')))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "您可以从此 LLMResult 中恢复诸如令牌使用之类的内容："
      ],
      "metadata": {
        "id": "4vtJzH_aIzaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.llm_output['token_usage']\n",
        "# -> {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}"
      ],
      "metadata": {
        "id": "6Fpe2hwWIzUL",
        "outputId": "678eff60-0d5b-4f1d-f9ce-94dfe479a599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt_tokens': 57, 'completion_tokens': 19, 'total_tokens': 76}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 聊天提示模板\n",
        "与 LLM 类似，您可以通过使用MessagePromptTemplate. 您可以ChatPromptTemplate从一个或多个MessagePromptTemplates 构建一个。您可以使用ChatPromptTemplate's format_prompt– 这将返回一个PromptValue，您可以将其转换为字符串或Message对象，具体取决于您是否要将格式化值用作 llm 或聊天模型的输入。\n",
        "\n",
        "为方便起见，from_template模板上公开了一个方法。如果您要使用此模板，它会是这样的："
      ],
      "metadata": {
        "id": "LPJ4RyMEIzOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# get a chat completion from the formatted messages\n",
        "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"Chinese\", text=\"I love programming.\").to_messages())\n",
        "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
      ],
      "metadata": {
        "id": "BC9UkilsIzHl",
        "outputId": "e9accead-bdad-42cc-bb15-4ac238c0742c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(lc_kwargs={'content': '我喜欢编程。'}, content='我喜欢编程。', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 带有聊天模型的链\n",
        "上一节中讨论的内容LLMChain也可以与聊天模型一起使用："
      ],
      "metadata": {
        "id": "Z9NU3xELIzCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "chain.run(input_language=\"English\", output_language=\"Chinese\", text=\"I love programming.\")\n",
        "# -> \"J'aime programmer.\""
      ],
      "metadata": {
        "id": "AR45VwbQIy7d",
        "outputId": "d3d76a01-ab70-4861-e190-c3ac9c84a53e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'我喜欢编程。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 具有聊天模型的代理\n",
        "代理也可以与聊天模型一起使用，您可以初始化一个AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION作为代理类型。"
      ],
      "metadata": {
        "id": "WkT6818VIy1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# First, let's load the language model we're going to use to control the agent.\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
        "llm = OpenAI(temperature=0)\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now let's test it out!\n",
        "agent.run(\"迪丽热巴出演过几次电影？ 这个数字乘以她当前年龄的 0.23 次幂是多少？\")"
      ],
      "metadata": {
        "id": "oBWth2ONIyuz",
        "outputId": "bb4237c6-eda6-40d6-bfb4-c9eec94e23e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mQuestion: 迪丽热巴出演过几次电影？ 这个数字乘以她当前年龄的 0.23 次幂是多少？\n",
            "\n",
            "Thought: I need to use a search engine to find out how many movies Dilraba Dilmurat has acted in and then use a calculator to perform the necessary calculations.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Search\",\n",
            "  \"action_input\": \"Dilraba Dilmurat movies\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mDilraba Dilmurat, known mononymously as Dilireba, is a Chinese actress, singer and model of Uyghur ethnicity.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mNow I need to search for the number of movies Dilraba Dilmurat has acted in and her current age.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Search\",\n",
            "  \"action_input\": \"Dilraba Dilmurat number of movies and age\"\n",
            "}\n",
            "```\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mDilraba Dilmurat (Uyghur: دىلرەبا دىلمۇرات; born June 3, 1992), known mononymously as ... In 2018, Dilraba starred in the romantic comedy film 21 Karat.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now have the information I need to calculate the answer. I will use a calculator to perform the necessary calculations.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Calculator\",\n",
            "  \"action_input\": \"21^0.23 * 29\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 58.412855303074444\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe final answer is 58.412855303074444.\n",
            "Final Answer: 58.412855303074444\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'58.412855303074444'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 内存：将状态添加到链和代理中\n",
        "您可以将内存与使用聊天模型初始化的链和代理一起使用。这与 Memory for LLMs 的主要区别在于，我们可以将它们保留为自己唯一的内存对象，而不是试图将所有以前的消息压缩成一个字符串。"
      ],
      "metadata": {
        "id": "h4Kjl0_zIyor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
        "\n",
        "conversation.predict(input=\"你好\")\n",
        "# -> 'Hello! How can I assist you today?'"
      ],
      "metadata": {
        "id": "wAi-YXEEIyie",
        "outputId": "6a844464-5b6f-4c9d-eae8-0a9bca1e7027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'你好！我是一名AI语言模型，很高兴能和你交流。有什么我可以帮助你的吗？'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"我正在和 AI 聊天\")\n",
        "# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\""
      ],
      "metadata": {
        "id": "a9xTJlFwSnkM",
        "outputId": "82e1673e-479f-411a-e368-9501353387ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'是的，你正在和我这个AI语言模型聊天。我可以回答你的问题，提供信息，或者只是和你闲聊。有什么你想问我的吗？'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"介绍下你自己\")\n",
        "# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\""
      ],
      "metadata": {
        "id": "mn42eb8OSeW1",
        "outputId": "9b4c87ff-8448-4d28-98ad-13803c33ddc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'当然可以！我是一名AI语言模型，由OpenAI公司开发。我被训练来理解和生成自然语言，可以用来回答问题、提供信息、生成文本等等。我使用的技术是深度学习，通过大量的数据和算法来学习自然语言的规律和模式。虽然我只是一种人工智能，但我可以模拟人类的思维和语言能力，帮助人们解决问题和交流。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"我刚才在干嘛\")\n",
        "# 您刚才说了一句中文：“我正在和 AI 聊天”。我猜您可能是想测试一下我的中文能力，或者只是想和我打个招呼。不过，无论您的目的是什么，我都很高兴能够和您交流。\n",
        "# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\""
      ],
      "metadata": {
        "id": "7l0O4XJnSh01",
        "outputId": "fa703126-05c8-448d-cd51-a55763b8ec60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'很抱歉，我不知道你刚才在干什么，因为我没有访问你的设备或者网络。作为一个AI语言模型，我只能回答你的问题或者提供信息，但我无法获取你的个人信息或者监视你的活动。如果你有任何需要帮助的问题，我会尽力回答。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 如何创建 ChatGPT 克隆\n",
        "该链通过结合 (1) 特定提示和 (2) 内存概念来复制 ChatGPT。\n",
        "\n",
        "展示 https://www.engraved.blog/building-a-virtual-machine-inside/ 中的示例"
      ],
      "metadata": {
        "id": "1YvjIBVeIydU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain, LLMChain, PromptTemplate\n",
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "\n",
        "template = \"\"\"Assistant is a large language model trained by OpenAI.\n",
        "\n",
        "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
        "\n",
        "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
        "\n",
        "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
        "\n",
        "{history}\n",
        "Human: {human_input}\n",
        "Assistant:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"history\", \"human_input\"],\n",
        "    template=template\n",
        ")\n",
        "\n",
        "\n",
        "chatgpt_chain = LLMChain(\n",
        "    llm=OpenAI(temperature=0),\n",
        "    prompt=prompt,\n",
        "    verbose=True,\n",
        "    memory=ConversationBufferWindowMemory(k=2),\n",
        ")\n",
        "\n",
        "output = chatgpt_chain.predict(human_input=\"I want you to act as a Linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. Do not write explanations. Do not type commands unless I instruct you to do so. When I need to tell you something in English I will do so by putting text inside curly brackets {like this}. My first command is pwd.\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "pMTQV4VdIyW7",
        "outputId": "8cb7a255-a037-462b-c0a3-c1048ac2f2a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAssistant is a large language model trained by OpenAI.\n",
            "\n",
            "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
            "\n",
            "\n",
            "Human: I want you to act as a Linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. Do not write explanations. Do not type commands unless I instruct you to do so. When I need to tell you something in English I will do so by putting text inside curly brackets {like this}. My first command is pwd.\n",
            "Assistant:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "```\n",
            "/home/user\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tqlhCUY9IyQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = chatgpt_chain.predict(human_input=\"ls ~\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "J4irqoLcIyKT",
        "outputId": "73010bce-62b4-4385-ef75-f39aed8bb9be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAssistant is a large language model trained by OpenAI.\n",
            "\n",
            "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
            "\n",
            "Human: I want you to act as a Linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. Do not write explanations. Do not type commands unless I instruct you to do so. When I need to tell you something in English I will do so by putting text inside curly brackets {like this}. My first command is pwd.\n",
            "AI: \n",
            "```\n",
            "/home/user\n",
            "```\n",
            "Human: ls ~\n",
            "Assistant:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "```\n",
            "Desktop/ Documents/ Downloads/ Music/ Pictures/ Public/ Templates/ Videos/\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xb_a-rvCIyCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = chatgpt_chain.predict(human_input=\"cd ~\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "n3AEaohKIx2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tePsbpANIxck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = chatgpt_chain.predict(human_input=\"\"\"echo -e \"x=lambda y:y*5+3;print('Result:' + str(x(6)))\" > run.py && python3 run.py\"\"\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "og888XX8IwT8",
        "outputId": "47119dc6-646c-41d3-9afc-37e6bcba681d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mAssistant is a large language model trained by OpenAI.\n",
            "\n",
            "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n",
            "\n",
            "Human: I want you to act as a Linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. Do not write explanations. Do not type commands unless I instruct you to do so. When I need to tell you something in English I will do so by putting text inside curly brackets {like this}. My first command is pwd.\n",
            "AI: \n",
            "```\n",
            "/home/user\n",
            "```\n",
            "Human: ls ~\n",
            "AI: \n",
            "```\n",
            "Desktop/ Documents/ Downloads/ Music/ Pictures/ Public/ Templates/ Videos/\n",
            "```\n",
            "Human: echo -e \"x=lambda y:y*5+3;print('Result:' + str(x(6)))\" > run.py && python3 run.py\n",
            "Assistant:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " \n",
            "```\n",
            "Result:33\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EwDKTRNfSLKm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "此笔记本展示了使用代理复制 MRKL 链。\n",
        "\n",
        "这使用示例 Chinook 数据库。要设置它，请按照 https://database.guide/2-sample-databases-sqlite/ 上的说明进行操作，将.db文件放在该存储库根目录下的 notebooks 文件夹中。"
      ],
      "metadata": {
        "id": "dKHUznuqS_xI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title chinook database\n",
        "\n",
        "### useful: download and extract chinook sample DB\n",
        "import urllib.request\n",
        "import zipfile\n",
        "from functools import partial\n",
        "import os\n",
        "\n",
        "chinook_url = 'https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite'\n",
        "chinook_url = 'http://www.sqlitetutorial.net/wp-content/uploads/2018/03/chinook.zip'\n",
        "if not os.path.exists('chinook.zip'):\n",
        "    print('downloading chinook.zip ', end='')\n",
        "    with urllib.request.urlopen(chinook_url) as response:\n",
        "        with open('chinook.zip', 'wb') as f:\n",
        "            for data in iter(partial(response.read, 4*1024), b''):\n",
        "                print('.', end='', flush=True)\n",
        "                f.write(data)\n",
        "\n",
        "zipfile.ZipFile('chinook.zip').extractall()\n",
        "assert os.path.exists('chinook.db')"
      ],
      "metadata": {
        "id": "GSPuGIlSSKrn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://cbt-cdn.oss-cn-hangzhou.aliyuncs.com/Chinook_Sqlite.sqlite"
      ],
      "metadata": {
        "id": "jBi6FVhGS0iX",
        "outputId": "254067b1-46f9-445b-ce2d-556bb89fa170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-13 15:50:52--  https://cbt-cdn.oss-cn-hangzhou.aliyuncs.com/Chinook_Sqlite.sqlite\n",
            "Resolving cbt-cdn.oss-cn-hangzhou.aliyuncs.com (cbt-cdn.oss-cn-hangzhou.aliyuncs.com)... 47.110.23.43\n",
            "Connecting to cbt-cdn.oss-cn-hangzhou.aliyuncs.com (cbt-cdn.oss-cn-hangzhou.aliyuncs.com)|47.110.23.43|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1067008 (1.0M) [application/octet-stream]\n",
            "Saving to: ‘Chinook_Sqlite.sqlite’\n",
            "\n",
            "Chinook_Sqlite.sqli 100%[===================>]   1.02M   579KB/s    in 1.8s    \n",
            "\n",
            "2023-06-13 15:50:56 (579 KB/s) - ‘Chinook_Sqlite.sqlite’ saved [1067008/1067008]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "id": "PoVflQfjct0P",
        "outputId": "7e078419-7357-4346-b6a5-3e8227d64267",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chinook.db  Chinook.db  chinook.zip  \u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMMathChain, OpenAI, SerpAPIWrapper, SQLDatabase, SQLDatabaseChain\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain.agents import AgentType"
      ],
      "metadata": {
        "id": "mCnHAHp2UH70"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "search = SerpAPIWrapper()\n",
        "llm_math_chain = LLMMathChain(llm=llm, verbose=True)\n",
        "db = SQLDatabase.from_uri(\"sqlite:///./Chinook_Sqlite.sqlite.db\")\n",
        "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)\n",
        "tools = [\n",
        "    Tool(\n",
        "        name = \"Search\",\n",
        "        func=search.run,\n",
        "        description=\"用来回答有关当前事件的问题。你应该问有针对性的问题\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Calculator\",\n",
        "        func=llm_math_chain.run,\n",
        "        description=\"用来回答数学问题\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"FooBar DB\",\n",
        "        func=db_chain.run,\n",
        "        description=\"用来回答关于FooBar的问题。\"\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "Xot6rTbsTDqi",
        "outputId": "d57990b2-2847-49b8-a2ed-a0cdc68f8545",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/chains/llm_math/base.py:50: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mrkl = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
      ],
      "metadata": {
        "id": "fGPxXd9aZ6QR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mrkl.run(\"谁是小李子的女朋友?她现在的年龄的0.43次方是多少?\")"
      ],
      "metadata": {
        "id": "bAG4CKRKZ8pp",
        "outputId": "55baf7fc-5325-43e8-8b1f-25c9d074f5e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m 我需要找到小李子的女朋友的名字，然后用计算器来计算她的年龄的0.43次方。\n",
            "Action: Search\n",
            "Action Input: 小李子的女朋友\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mEden Polani出生于2004年，今年十九岁，Instagram上有将近20万粉丝。 她常在网络上分享美妆和服饰的视频，除了模特身份，也是一个小有名气的网红博主。 虽然此事还没有实锤，但是以小李子更换女友的频率和精神面貌来看，下一任女友基本上已经定了。\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m 我知道了Eden Polani是小李子的女朋友，现在我需要用计算器来计算她的年龄的0.43次方。\n",
            "Action: Calculator\n",
            "Action Input: 19^0.43\u001b[0m19^0.43\u001b[32;1m\u001b[1;3m```text\n",
            "19**0.43\n",
            "```\n",
            "...numexpr.evaluate(\"19**0.43\")...\n",
            "\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m3.547023357958959\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 3.547023357958959\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m 我现在知道了答案\n",
            "Final Answer: Eden Polani的年龄的0.43次方是3.547023357958959。\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Eden Polani的年龄的0.43次方是3.547023357958959。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mrkl.run(\"最近发行专辑《the Storm Before the Calm》的艺术家的全名是什么\")"
      ],
      "metadata": {
        "id": "GcY3XHmnaALx",
        "outputId": "d034e301-0792-4651-c728-c8ceb11ef1fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m 我应该搜索这张专辑的信息\n",
            "Action: Search\n",
            "Action Input: 《the Storm Before the Calm》 专辑\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mThe Storm Before the Calm is the tenth studio album by Canadian-American singer-songwriter Alanis Morissette, released June 17, 2022, via Epiphany Music and Thirty Tigers, as well as by RCA Records in Europe.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m 我现在知道最终答案了\n",
            "Final Answer: Alanis Morissette\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Alanis Morissette'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}