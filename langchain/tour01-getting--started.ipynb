{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanland/colab-notebooks/blob/main/langchain/tour01-getting--started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg8LXAe8mt-G"
      },
      "source": [
        "# 快速入门指南\n",
        "本教程让您快速了解如何使用 LangChain 构建端到端语言模型应用程序。\n",
        "\n",
        "安装\n",
        "首先，使用以下命令安装 LangChain："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ijIJmBFLebe3",
        "outputId": "4458dcba-7b68-4b60-ed82-1f3f27988d09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.198-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.10)\n",
            "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0.0,>=4.0.0 (from langchain)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.8-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk>=0.0.7 (from langchain)\n",
            "  Downloading langchainplus_sdk-0.0.8-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.7)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.19.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, multidict, marshmallow, frozenlist, async-timeout, yarl, typing-inspect, openapi-schema-pydantic, marshmallow-enum, langchainplus-sdk, aiosignal, dataclasses-json, aiohttp, langchain\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 dataclasses-json-0.5.8 frozenlist-1.3.3 langchain-0.0.198 langchainplus-sdk-0.0.8 marshmallow-3.19.0 marshmallow-enum-1.5.1 multidict-6.0.4 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0 yarl-1.9.2\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn_LkaeUmFJQ"
      },
      "source": [
        "##环境设置\n",
        "使用 LangChain 通常需要与一个或多个模型提供者、数据存储、api 等集成。\n",
        "\n",
        "对于这个例子，我们将使用 OpenAI 的 API，所以我们首先需要安装他们的 SDK："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l4XypmLmfJNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c80a90-8c5f-43c6-a7d9-8363011d5bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.8\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "然后我们需要在终端中设置环境变量。"
      ],
      "metadata": {
        "id": "WBoWlgRsHiJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export OPENAI_API_KEY=\"...\""
      ],
      "metadata": {
        "id": "wpx9MUD8Hm-l"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "或者，您可以从 Jupyter notebook（或 Python 脚本）中执行此操作："
      ],
      "metadata": {
        "id": "x5fT6vg1ICIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"...\""
      ],
      "metadata": {
        "id": "pqJwYvOBICpc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "如果要动态设置 API 密钥，可以在启动 OpenAI 类时使用 openai_api_key 参数——例如，每个用户的 API 密钥。"
      ],
      "metadata": {
        "id": "SXHunVn9IDEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(openai_api_key=\"OPENAI_API_KEY\")"
      ],
      "metadata": {
        "id": "KO4xBAJuIDfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 构建语言模型应用程序：LLM \n",
        "现在我们已经安装了 LangChain 并设置了我们的环境，我们可以开始构建我们的语言模型应用程序了。\n",
        "\n",
        "LangChain 提供了很多可以用来构建语言模型应用的模块。模块可以组合起来创建更复杂的应用程序，或者单独用于简单的应用程序。\n",
        "\n",
        "LangChain 最基本的构建块是在某些输入上调用 LLM。让我们通过一个简单的例子来说明如何做到这一点。为此，假设我们正在构建一项服务，该服务会根据公司的产品生成公司名称。\n",
        "\n",
        "为此，我们首先需要导入 LLM 包装器。"
      ],
      "metadata": {
        "id": "eX3B8WgJIagX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "vAY3ye9PILUA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "然后我们可以用任何参数初始化包装器。在此示例中，我们可能希望输出更加随机，因此我们将使用高温对其进行初始化。"
      ],
      "metadata": {
        "id": "t9MsPv0cImBc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.9)"
      ],
      "metadata": {
        "id": "YE-mpF5_I1VV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们现在可以根据一些输入调用它！"
      ],
      "metadata": {
        "id": "C5WOWbKcI044"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"帮我给一家做 AI 服务的科技公司起一个中文名和英文名\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgchcTSJI0fT",
        "outputId": "b5b597bf-1ae2-4360-9775-a18574faee38"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "中文名：智能联盟\n",
            "英文名：Intelligent Alliance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"帮我给一家做 AI 服务的科技公司起一个中文名和英文名\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "outputId": "4ebb3623-9dd3-4498-c26d-93f76dc566ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhY4BX7nJwW2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "中文名：智博科技\n",
            "英文名：InteBridge Technology\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 提示模板：管理 LLM 的提示\n",
        "获得LLM是重要的第一步，但这仅仅是个开始。通常，当您在应用程序中使用 LLM 时，您不会将用户输入直接发送到 LLM。相反，您可能正在接受用户输入并构建提示，然后将其发送给 LLM。\n",
        "\n",
        "例如，在前面的示例中，我们传入的文本被硬编码为要求一家公司的名称。在这个假想的服务中，我们想要做的是只接受描述公司所做的用户输入，然后用该信息格式化提示。\n",
        "\n",
        "使用 LangChain 很容易做到这一点！\n",
        "\n",
        "首先让我们定义提示模板："
      ],
      "metadata": {
        "id": "Zwy7fX7uI0Y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"帮我给一家做 {product} 服务的科技公司起一个中文名和英文名\",\n",
        ")"
      ],
      "metadata": {
        "id": "r5_C6CjZI0Sr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在让我们看看这是如何工作的！我们可以调用该.format方法对其进行格式化。"
      ],
      "metadata": {
        "id": "MBIXD9e0I0ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(prompt.format(product=\"相册\"))"
      ],
      "metadata": {
        "id": "woaMouCiI0Gq",
        "outputId": "eb64dd65-a570-43c8-ebdb-fd520dc56002",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "帮我给一家做 相册 服务的科技公司起一个中文名和英文名\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chains：在多步骤工作流程中结合 LLM 和提示\n",
        "到目前为止，我们一直在单独使用 PromptTemplate 和 LLM 原语。但是，当然，真正的应用程序不仅仅是一个原语，而是它们的组合。\n",
        "\n",
        "LangChain 中的一条链由链接组成，链接可以是像 LLM 这样的原始链，也可以是其他链。\n",
        "\n",
        "最核心的链类型是 LLMChain，它由 PromptTemplate 和 LLM 组成。\n",
        "\n",
        "扩展前面的示例，我们可以构造一个 LLMChain，它接受用户输入，使用 PromptTemplate 对其进行格式化，然后将格式化的响应传递给 LLM。"
      ],
      "metadata": {
        "id": "RETA5ff3I0A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(temperature=0.9)\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\"],\n",
        "    template=\"帮我给一家做 {product} 服务的科技公司起一个中文名和英文名\",\n",
        ")"
      ],
      "metadata": {
        "id": "5vaa9NjLIz6L"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们现在可以创建一个非常简单的链，它将接受用户输入，用它格式化提示，然后将它发送到 LLM："
      ],
      "metadata": {
        "id": "x0jCSH-2Iz0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "JY1j341DIzrW"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "现在我们可以仅指定产品来运行该链！"
      ],
      "metadata": {
        "id": "l2tmnIJ_Izl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(\"电子音乐\")"
      ],
      "metadata": {
        "id": "rz4o0Xl-Izfz",
        "outputId": "bd0e3987-8381-4095-fad8-8cf4f60c5ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\n中文名：节拍音乐 \\n英文名：Beats Music'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 代理：基于用户输入的动态调用链\n",
        "到目前为止，我们所看到的链条以预定的顺序运行。\n",
        "\n",
        "代理人不再这样做：他们使用 LLM 来确定采取哪些行动以及采取何种顺序。动作可以是使用工具并观察其输出，也可以是返回给用户。\n",
        "\n",
        "如果使用得当，代理可以非常强大。在本教程中，我们将向您展示如何通过最简单、最高级别的 API 轻松使用代理。\n",
        "\n",
        "为了加载代理，您应该了解以下概念：\n",
        "\n",
        "- 工具：执行特定任务的功能。这可以是：Google 搜索、数据库查找、Python REPL、其他链。工具的接口目前是一个函数，期望将字符串作为输入，将字符串作为输出。\n",
        "- LLM：为代理提供支持的语言模型。\n",
        "- 代理：要使用的代理。这应该是一个引用支持代理类的字符串。由于本笔记本侧重于最简单、最高级别的 API，因此仅涵盖使用标准支持的代理。如果您想实施自定义代理，请参阅自定义代理的文档（即将推出）。\n",
        "\n",
        "**代理**：有关受支持代理及其规范的列表，请参见此处。\n",
        "\n",
        "**工具**：有关预定义工具及其规格的列表，请参见此处。\n",
        "\n",
        "对于此示例，您还需要安装 SerpAPI Python 包。"
      ],
      "metadata": {
        "id": "PvsCOIAdLD9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results"
      ],
      "metadata": {
        "id": "UxTdL-_5LD3s",
        "outputId": "32a4712c-14bf-4ac5-b345-b24de76f1bca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32002 sha256=df15ebbbb5079f756feaa19d6a3e7e002ab78287112ca769d09d05dd14379316\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "并设置相应的环境变量。"
      ],
      "metadata": {
        "id": "NGo33dm0LDx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"...\""
      ],
      "metadata": {
        "id": "jyTD0fS6LDr-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "现在我们可以开始了！"
      ],
      "metadata": {
        "id": "yPRIJtoOLDlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# First, let's load the language model we're going to use to control the agent.\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now let's test it out!\n",
        "agent.run(\"上海昨天的高温是多少度？ 这个数字的 .023 次方是多少\")"
      ],
      "metadata": {
        "id": "qmXHpyHtLDgE",
        "outputId": "8ad513ff-f69d-4108-9986-a4be5567c030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m I need to find out the temperature in Shanghai yesterday and then calculate the .023 power of that number\n",
            "Action: Search\n",
            "Action Input: \"Shanghai temperature yesterday\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mhttps://www.timeanddate.com/weather/china/shanghai/historic\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I can use the calculator to calculate the .023 power\n",
            "Action: Calculator\n",
            "Action Input: \"Temperature in Shanghai yesterday (34°C) ^ .023\"\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 1.084486162715683\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The .023 power of the temperature in Shanghai yesterday (34°C) is 1.084486162715683.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The .023 power of the temperature in Shanghai yesterday (34°C) is 1.084486162715683.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 内存：将状态添加到链和代理中\n",
        "到目前为止，我们所经历的所有链和代理都是无状态的。但通常，您可能希望链或代理具有某种“记忆”概念，以便它可以记住有关其先前交互的信息。最清晰和简单的例子是在设计聊天机器人时——您希望它记住以前的消息，以便它可以使用来自该消息的上下文来进行更好的对话。这将是一种“短期记忆”。在更复杂的方面，你可以想象一个链/代理随着时间的推移记住关键信息——这将是一种“长期记忆”。有关后者的更具体想法，请参阅这篇很棒的论文。\n",
        "\n",
        "LangChain 专门为此提供了几个专门创建的链。本笔记本介绍了如何使用这些链中的一个 (the ConversationChain) 和两种不同类型的内存。\n",
        "\n",
        "默认情况下，ConversationChain有一种简单类型的内存，可以记住所有以前的输入/输出并将它们添加到传递的上下文中。让我们来看看使用这个链（设置verbose=True以便我们可以看到提示）。"
      ],
      "metadata": {
        "id": "9tOrbk56LDal"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain\n",
        "\n",
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)\n",
        "\n",
        "output = conversation.predict(input=\"你好\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "TXjGO9mrLDUb",
        "outputId": "040e9885-df2a-49a1-d795-e3b73568e2f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: 你好\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " 你好！很高兴见到你！我是一个智能AI，我可以回答你的问题，或者我们可以聊聊天？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = conversation.predict(input=\"我正在跟 AI 聊天呢。\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "geGbIHxcNnmJ",
        "outputId": "6af50c4f-ab8d-4ffb-ca37-350dd6008ad5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: 你好\n",
            "AI:  你好！很高兴见到你！我是一个智能AI，我可以回答你的问题，或者我们可以聊聊天？\n",
            "Human: 我正在跟 AI 聊天呢。\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            " 哦，真的吗？这太棒了！我很高兴能和你聊天！有什么可以帮助你的吗？\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 构建语言模型应用程序：聊天模型\n",
        "同样，您可以使用聊天模型而不是 LLM。聊天模型是语言模型的变体。虽然聊天模型在底层使用语言模型，但它们公开的接口有点不同：它们公开的不是“文本输入、文本输出”API，而是“聊天消息”作为输入和输出的接口。\n",
        "\n",
        "聊天模型 API 相当新，所以我们仍在寻找正确的抽象。\n",
        "\n",
        "## 从聊天模型中获取消息完成\n",
        "您可以通过将一条或多条消息传递给聊天模型来获得聊天完成。响应将是一条消息。LangChain 目前支持的消息类型有AIMessage, HumanMessage, SystemMessage, 和ChatMessage–ChatMessage接受任意角色参数。大多数时候，您只会处理HumanMessage、AIMessage和SystemMessage。\n"
      ],
      "metadata": {
        "id": "-vRLWNJ3LDN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)"
      ],
      "metadata": {
        "id": "CdVLwvN8LDIE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "您可以通过传递一条消息来完成。"
      ],
      "metadata": {
        "id": "lNdZV9ZdLDB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat([HumanMessage(content=\"Translate this sentence from English to Chinese. I love programming.\")])\n",
        "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
      ],
      "metadata": {
        "id": "v5XGFwTsLC8E",
        "outputId": "b851d93a-6f9e-4714-925a-58882feec5c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(lc_kwargs={'content': '我喜欢编程。'}, content='我喜欢编程。', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "您还可以为 OpenAI 的 gpt-3.5-turbo 和 gpt-4 模型传入多条消息。"
      ],
      "metadata": {
        "id": "4OKkvs5PLC2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are a helpful assistant that translates Chinese to English.\"),\n",
        "    HumanMessage(content=\"我喜欢编程。\")\n",
        "]\n",
        "chat(messages)\n",
        "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
      ],
      "metadata": {
        "id": "ul8NnIANLCvu",
        "outputId": "39225ebf-b567-481f-9a24-1cb9492dc005",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(lc_kwargs={'content': '\"I like programming.\"'}, content='\"I like programming.\"', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "您可以更进一步，使用 为多组消息生成补全generate。这将返回LLMResult带有附加message参数的 ："
      ],
      "metadata": {
        "id": "Zo0-E7BvLCn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_messages = [\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to Chinese.\"),\n",
        "        HumanMessage(content=\"I love programming.\")\n",
        "    ],\n",
        "    [\n",
        "        SystemMessage(content=\"You are a helpful assistant that translates English to Chinese.\"),\n",
        "        HumanMessage(content=\"I love artificial intelligence.\")\n",
        "    ],\n",
        "]\n",
        "result = chat.generate(batch_messages)\n",
        "result"
      ],
      "metadata": {
        "id": "Kvq3FcXYLCdH",
        "outputId": "5a36debe-32b6-48f5-f14e-83213ef3ca8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LLMResult(generations=[[ChatGeneration(lc_kwargs={'message': AIMessage(lc_kwargs={'content': '我喜欢编程。'}, content='我喜欢编程。', additional_kwargs={}, example=False)}, text='我喜欢编程。', generation_info=None, message=AIMessage(lc_kwargs={'content': '我喜欢编程。'}, content='我喜欢编程。', additional_kwargs={}, example=False))], [ChatGeneration(lc_kwargs={'message': AIMessage(lc_kwargs={'content': '我喜欢人工智能。'}, content='我喜欢人工智能。', additional_kwargs={}, example=False)}, text='我喜欢人工智能。', generation_info=None, message=AIMessage(lc_kwargs={'content': '我喜欢人工智能。'}, content='我喜欢人工智能。', additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 57, 'completion_tokens': 19, 'total_tokens': 76}, 'model_name': 'gpt-3.5-turbo'}, run=RunInfo(run_id=UUID('7212ce06-5e1f-43d7-8e12-498d977e1e3a')))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "您可以从此 LLMResult 中恢复诸如令牌使用之类的内容："
      ],
      "metadata": {
        "id": "4vtJzH_aIzaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.llm_output['token_usage']\n",
        "# -> {'prompt_tokens': 57, 'completion_tokens': 20, 'total_tokens': 77}"
      ],
      "metadata": {
        "id": "6Fpe2hwWIzUL",
        "outputId": "678eff60-0d5b-4f1d-f9ce-94dfe479a599",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt_tokens': 57, 'completion_tokens': 19, 'total_tokens': 76}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 聊天提示模板\n",
        "与 LLM 类似，您可以通过使用MessagePromptTemplate. 您可以ChatPromptTemplate从一个或多个MessagePromptTemplates 构建一个。您可以使用ChatPromptTemplate's format_prompt– 这将返回一个PromptValue，您可以将其转换为字符串或Message对象，具体取决于您是否要将格式化值用作 llm 或聊天模型的输入。\n",
        "\n",
        "为方便起见，from_template模板上公开了一个方法。如果您要使用此模板，它会是这样的："
      ],
      "metadata": {
        "id": "LPJ4RyMEIzOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "# get a chat completion from the formatted messages\n",
        "chat(chat_prompt.format_prompt(input_language=\"English\", output_language=\"Chinese\", text=\"I love programming.\").to_messages())\n",
        "# -> AIMessage(content=\"J'aime programmer.\", additional_kwargs={})"
      ],
      "metadata": {
        "id": "BC9UkilsIzHl",
        "outputId": "e9accead-bdad-42cc-bb15-4ac238c0742c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(lc_kwargs={'content': '我喜欢编程。'}, content='我喜欢编程。', additional_kwargs={}, example=False)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 带有聊天模型的链\n",
        "上一节中讨论的内容LLMChain也可以与聊天模型一起使用："
      ],
      "metadata": {
        "id": "Z9NU3xELIzCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template = \"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "chain.run(input_language=\"English\", output_language=\"Chinese\", text=\"I love programming.\")\n",
        "# -> \"J'aime programmer.\""
      ],
      "metadata": {
        "id": "AR45VwbQIy7d",
        "outputId": "d3d76a01-ab70-4861-e190-c3ac9c84a53e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'我喜欢编程。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 具有聊天模型的代理\n",
        "代理也可以与聊天模型一起使用，您可以初始化一个AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION作为代理类型。"
      ],
      "metadata": {
        "id": "WkT6818VIy1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# First, let's load the language model we're going to use to control the agent.\n",
        "chat = ChatOpenAI(temperature=0)\n",
        "\n",
        "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
        "llm = OpenAI(temperature=0)\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# Now let's test it out!\n",
        "agent.run(\"迪丽热巴出演过几次电影？ 这个数字乘以她当前年龄的 0.23 次幂是多少？\")"
      ],
      "metadata": {
        "id": "oBWth2ONIyuz",
        "outputId": "bb4237c6-eda6-40d6-bfb4-c9eec94e23e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.callbacks.manager:Error in on_chain_start callback: 'name'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3mQuestion: 迪丽热巴出演过几次电影？ 这个数字乘以她当前年龄的 0.23 次幂是多少？\n",
            "\n",
            "Thought: I need to use a search engine to find out how many movies Dilraba Dilmurat has acted in and then use a calculator to perform the necessary calculations.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Search\",\n",
            "  \"action_input\": \"Dilraba Dilmurat movies\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mDilraba Dilmurat, known mononymously as Dilireba, is a Chinese actress, singer and model of Uyghur ethnicity.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mNow I need to search for the number of movies Dilraba Dilmurat has acted in and her current age.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Search\",\n",
            "  \"action_input\": \"Dilraba Dilmurat number of movies and age\"\n",
            "}\n",
            "```\n",
            "\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mDilraba Dilmurat (Uyghur: دىلرەبا دىلمۇرات; born June 3, 1992), known mononymously as ... In 2018, Dilraba starred in the romantic comedy film 21 Karat.\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mI now have the information I need to calculate the answer. I will use a calculator to perform the necessary calculations.\n",
            "\n",
            "Action:\n",
            "```\n",
            "{\n",
            "  \"action\": \"Calculator\",\n",
            "  \"action_input\": \"21^0.23 * 29\"\n",
            "}\n",
            "```\n",
            "\n",
            "\u001b[0m\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 58.412855303074444\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3mThe final answer is 58.412855303074444.\n",
            "Final Answer: 58.412855303074444\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'58.412855303074444'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 内存：将状态添加到链和代理中\n",
        "您可以将内存与使用聊天模型初始化的链和代理一起使用。这与 Memory for LLMs 的主要区别在于，我们可以将它们保留为自己唯一的内存对象，而不是试图将所有以前的消息压缩成一个字符串。"
      ],
      "metadata": {
        "id": "h4Kjl0_zIyor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate, \n",
        "    MessagesPlaceholder, \n",
        "    SystemMessagePromptTemplate, \n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\"),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
        "])\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
        "\n",
        "conversation.predict(input=\"你好\")\n",
        "# -> 'Hello! How can I assist you today?'"
      ],
      "metadata": {
        "id": "wAi-YXEEIyie",
        "outputId": "6a844464-5b6f-4c9d-eae8-0a9bca1e7027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'你好！我是一名AI语言模型，很高兴能和你交流。有什么我可以帮助你的吗？'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"我正在和 AI 聊天\")\n",
        "# -> \"That sounds like fun! I'm happy to chat with you. Is there anything specific you'd like to talk about?\""
      ],
      "metadata": {
        "id": "a9xTJlFwSnkM",
        "outputId": "82e1673e-479f-411a-e368-9501353387ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'是的，你正在和我这个AI语言模型聊天。我可以回答你的问题，提供信息，或者只是和你闲聊。有什么你想问我的吗？'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"介绍下你自己\")\n",
        "# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\""
      ],
      "metadata": {
        "id": "mn42eb8OSeW1",
        "outputId": "9b4c87ff-8448-4d28-98ad-13803c33ddc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'当然可以！我是一名AI语言模型，由OpenAI公司开发。我被训练来理解和生成自然语言，可以用来回答问题、提供信息、生成文本等等。我使用的技术是深度学习，通过大量的数据和算法来学习自然语言的规律和模式。虽然我只是一种人工智能，但我可以模拟人类的思维和语言能力，帮助人们解决问题和交流。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"我刚才在干嘛\")\n",
        "# 您刚才说了一句中文：“我正在和 AI 聊天”。我猜您可能是想测试一下我的中文能力，或者只是想和我打个招呼。不过，无论您的目的是什么，我都很高兴能够和您交流。\n",
        "# -> \"Sure! I am an AI language model created by OpenAI. I was trained on a large dataset of text from the internet, which allows me to understand and generate human-like language. I can answer questions, provide information, and even have conversations like this one. Is there anything else you'd like to know about me?\""
      ],
      "metadata": {
        "id": "7l0O4XJnSh01",
        "outputId": "fa703126-05c8-448d-cd51-a55763b8ec60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'很抱歉，我不知道你刚才在干什么，因为我没有访问你的设备或者网络。作为一个AI语言模型，我只能回答你的问题或者提供信息，但我无法获取你的个人信息或者监视你的活动。如果你有任何需要帮助的问题，我会尽力回答。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1YvjIBVeIydU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pMTQV4VdIyW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tqlhCUY9IyQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J4irqoLcIyKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Xb_a-rvCIyCq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n3AEaohKIx2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tePsbpANIxck"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "og888XX8IwT8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}