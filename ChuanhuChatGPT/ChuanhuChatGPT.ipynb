{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanland/colab-notebooks/blob/main/ChuanhuChatGPT/ChuanhuChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg8LXAe8mt-G"
      },
      "source": [
        "# **ChuanhuChatGPT: Lightweight and User-friendly Web-UI for LLMs**\n",
        "\n",
        "\n",
        "Streaming / Unlimited conversations / Save history / Preset prompts / Chat with files / Web search\n",
        "LaTeX rendering / Table rendering / Code highlighting\n",
        "Auto dark mode / Adaptive web interface / WeChat-like theme\n",
        "Multi-parameters tuning / Multi-API-Key support / Multi-user support\n",
        "Compatible with GPT-4 / Local deployment for LLMs\n",
        "\n",
        "https://github.com/GaiZhenbiao/ChuanhuChatGPT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ijIJmBFLebe3",
        "outputId": "0302a007-fe0d-4d1a-bff1-ebbad6a52b91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4, 15360 MiB, 15101 MiB\n"
          ]
        }
      ],
      "source": [
        "### make sure that CUDA is available in Edit -> Nootbook settings -> GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn_LkaeUmFJQ"
      },
      "source": [
        "## Installnation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l4XypmLmfJNw",
        "outputId": "31ff3a37-8f8f-4333-9eb3-d9d68b11ecc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ChuanhuChatGPT'...\n",
            "remote: Enumerating objects: 2849, done.\u001b[K\n",
            "remote: Counting objects: 100% (1450/1450), done.\u001b[K\n",
            "remote: Compressing objects: 100% (294/294), done.\u001b[K\n",
            "remote: Total 2849 (delta 1255), reused 1182 (delta 1156), pack-reused 1399\u001b[K\n",
            "Receiving objects: 100% (2849/2849), 1.38 MiB | 11.66 MiB/s, done.\n",
            "Resolving deltas: 100% (1931/1931), done.\n",
            "/content/ChuanhuChatGPT\n"
          ]
        }
      ],
      "source": [
        "!cd /content\n",
        "# !git clone https://github.com/GaiZhenbiao/ChuanhuChatGPT.git\n",
        "!git clone https://github.com/alanland/ChuanhuChatGPT.git\n",
        "%cd ChuanhuChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout main-local\n",
        "!git pull\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Qq7czOb89Qbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements_advanced.txt"
      ],
      "metadata": {
        "id": "xCvYGtjdBvZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatGLM-6B\n",
        "!pip install protobuf==3.20.0\n",
        "!pip install torch>=1.10\n",
        "!pip install gradio\n",
        "!pip install mdtex2html\n",
        "!pip install sentencepiece\n",
        "# end ChatGLM-6B"
      ],
      "metadata": {
        "id": "9a0-rzmTDJ8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y8F_z_-hd9w",
        "outputId": "0585f880-f0e8-4b56-b2b7-520822f2963a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "2023-06-01 13:58:39,211 [INFO] [models.py:562] 正在加载OpenAI模型: gpt-3.5-turbo\n",
            "2023-06-01 13:58:39,211 [INFO] [models.py:615] Model is set to:  gpt-3.5-turbo\n",
            "2023-06-01 13:58:39,394 [INFO] [utils.py:410] Your IP region: United States。\n",
            "2023-06-01 13:58:39,511 [INFO] [ChuanhuChatbot.py:460] \u001b[42m\n",
            "川虎的温馨提示：访问 http://localhost:7860 查看界面\u001b[0m\n",
            "Reloading javascript...\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://8806487def2479f47c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n",
            "2023-06-01 13:58:53,558 [INFO] [models.py:562] 正在加载OpenAI模型: gpt-3.5-turbo\n",
            "2023-06-01 13:58:53,559 [INFO] [models.py:615] Model is set to:  gpt-3.5-turbo\n",
            "2023-06-01 13:58:55,718 [INFO] [models.py:562] 正在加载OpenAI模型: gpt-3.5-turbo\n",
            "2023-06-01 13:58:55,718 [INFO] [models.py:615] Model is set to:  gpt-3.5-turbo\n",
            "2023-06-01 13:59:25,931 [INFO] [models.py:572] 正在加载ChatGLM模型: chatglm-6b-int4\n",
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "2023-06-01 13:59:34.548952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "No compiled kernel found.\n",
            "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.c\n",
            "Compiling gcc -O3 -fPIC -std=c99 /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.c -shared -o /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.so\n",
            "Load kernel : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.so\n",
            "Using quantization cache\n",
            "Applying quantization to glm layers\n",
            "2023-06-01 13:59:56,019 [INFO] [models.py:258] CUDA is available, using CUDA\n",
            "2023-06-01 14:00:03,351 [ERROR] [models.py:90] 获取API使用情况失败:API request failed with status code 401: {\n",
            "  \"error\": {\n",
            "    \"message\": \"Invalid authorization header\",\n",
            "    \"type\": \"server_error\",\n",
            "    \"param\": null,\n",
            "    \"code\": null\n",
            "  }\n",
            "}\n",
            "\n",
            "2023-06-01 14:00:04,021 [INFO] [models.py:615] Model is set to:  chatglm-6b-int4\n",
            "2023-06-01 14:00:04,445 [INFO] [base_model.py:373] 输入为：\u001b[34m请介绍下你自己\u001b[0m\n",
            "2023-06-01 14:00:28,574 [INFO] [base_model.py:437] 回答为：\u001b[34m我是一个名为 ChatGLM-6B 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。ChatGLM-6B 版本采用清华大学 KEG 实验室的 GLM-6B 模型，我的任务是针对用户的问题和要求提供适当的答复和支持。我是一\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!git pull\n",
        "!python ChuanhuChatbot.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}