{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanland/colab-notebooks/blob/main/ChuanhuChatGPT/ChuanhuChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg8LXAe8mt-G"
      },
      "source": [
        "# **ChuanhuChatGPT: Lightweight and User-friendly Web-UI for LLMs**\n",
        "\n",
        "\n",
        "Streaming / Unlimited conversations / Save history / Preset prompts / Chat with files / Web search\n",
        "LaTeX rendering / Table rendering / Code highlighting\n",
        "Auto dark mode / Adaptive web interface / WeChat-like theme\n",
        "Multi-parameters tuning / Multi-API-Key support / Multi-user support\n",
        "Compatible with GPT-4 / Local deployment for LLMs\n",
        "\n",
        "https://github.com/GaiZhenbiao/ChuanhuChatGPT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ijIJmBFLebe3",
        "outputId": "0302a007-fe0d-4d1a-bff1-ebbad6a52b91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4, 15360 MiB, 15101 MiB\n"
          ]
        }
      ],
      "source": [
        "### make sure that CUDA is available in Edit -> Nootbook settings -> GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn_LkaeUmFJQ"
      },
      "source": [
        "## Installnation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l4XypmLmfJNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ff3a37-8f8f-4333-9eb3-d9d68b11ecc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ChuanhuChatGPT'...\n",
            "remote: Enumerating objects: 2849, done.\u001b[K\n",
            "remote: Counting objects: 100% (1450/1450), done.\u001b[K\n",
            "remote: Compressing objects: 100% (294/294), done.\u001b[K\n",
            "remote: Total 2849 (delta 1255), reused 1182 (delta 1156), pack-reused 1399\u001b[K\n",
            "Receiving objects: 100% (2849/2849), 1.38 MiB | 11.66 MiB/s, done.\n",
            "Resolving deltas: 100% (1931/1931), done.\n",
            "/content/ChuanhuChatGPT\n"
          ]
        }
      ],
      "source": [
        "!cd /content\n",
        "# !git clone https://github.com/GaiZhenbiao/ChuanhuChatGPT.git\n",
        "!git clone https://github.com/alanland/ChuanhuChatGPT.git\n",
        "%cd ChuanhuChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git checkout main-local\n",
        "!git pull\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Qq7czOb89Qbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements_advanced.txt"
      ],
      "metadata": {
        "id": "xCvYGtjdBvZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ChatGLM-6B\n",
        "!pip install protobuf==3.20.0\n",
        "!pip install torch>=1.10\n",
        "!pip install gradio\n",
        "!pip install mdtex2html\n",
        "!pip install sentencepiece\n",
        "# end ChatGLM-6B"
      ],
      "metadata": {
        "id": "9a0-rzmTDJ8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y8F_z_-hd9w",
        "outputId": "8aa023fd-9846-447e-a29d-6af8ee6fa439",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already up to date.\n",
            "2023-06-01 13:58:39,211 [INFO] [models.py:562] 正在加载OpenAI模型: gpt-3.5-turbo\n",
            "2023-06-01 13:58:39,211 [INFO] [models.py:615] Model is set to:  gpt-3.5-turbo\n",
            "2023-06-01 13:58:39,394 [INFO] [utils.py:410] Your IP region: United States。\n",
            "2023-06-01 13:58:39,511 [INFO] [ChuanhuChatbot.py:460] \u001b[42m\n",
            "川虎的温馨提示：访问 http://localhost:7860 查看界面\u001b[0m\n",
            "Reloading javascript...\n",
            "Running on local URL:  http://127.0.0.1:7860\n",
            "Running on public URL: https://8806487def2479f47c.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n",
            "2023-06-01 13:58:53,558 [INFO] [models.py:562] 正在加载OpenAI模型: gpt-3.5-turbo\n",
            "2023-06-01 13:58:53,559 [INFO] [models.py:615] Model is set to:  gpt-3.5-turbo\n",
            "2023-06-01 13:58:55,718 [INFO] [models.py:562] 正在加载OpenAI模型: gpt-3.5-turbo\n",
            "2023-06-01 13:58:55,718 [INFO] [models.py:615] Model is set to:  gpt-3.5-turbo\n",
            "2023-06-01 13:59:25,931 [INFO] [models.py:572] 正在加载ChatGLM模型: chatglm-6b-int4\n",
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "2023-06-01 13:59:34.548952: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "No compiled kernel found.\n",
            "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.c\n",
            "Compiling gcc -O3 -fPIC -std=c99 /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.c -shared -o /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.so\n",
            "Load kernel : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.so\n",
            "Using quantization cache\n",
            "Applying quantization to glm layers\n",
            "2023-06-01 13:59:56,019 [INFO] [models.py:258] CUDA is available, using CUDA\n",
            "2023-06-01 14:00:03,351 [ERROR] [models.py:90] 获取API使用情况失败:API request failed with status code 401: {\n",
            "  \"error\": {\n",
            "    \"message\": \"Invalid authorization header\",\n",
            "    \"type\": \"server_error\",\n",
            "    \"param\": null,\n",
            "    \"code\": null\n",
            "  }\n",
            "}\n",
            "\n",
            "2023-06-01 14:00:04,021 [INFO] [models.py:615] Model is set to:  chatglm-6b-int4\n",
            "2023-06-01 14:00:04,445 [INFO] [base_model.py:373] 输入为：\u001b[34m请介绍下你自己\u001b[0m\n",
            "2023-06-01 14:00:28,574 [INFO] [base_model.py:437] 回答为：\u001b[34m我是一个名为 ChatGLM-6B 的人工智能助手，是基于清华大学 KEG 实验室和智谱 AI 公司于 2023 年共同训练的语言模型开发的。我的任务是针对用户的问题和要求提供适当的答复和支持。ChatGLM-6B 版本采用清华大学 KEG 实验室的 GLM-6B 模型，我的任务是针对用户的问题和要求提供适当的答复和支持。我是一\u001b[0m\n",
            "2023-06-01 14:02:57,970 [WARNING] [base_model.py:223] billing info not implemented, using default\n",
            "2023-06-01 14:02:58,432 [INFO] [base_model.py:373] 输入为：\u001b[34m怎么用 python 遍历元组\u001b[0m\n",
            "2023-06-01 14:03:13,147 [INFO] [config.py:175] 更新后的文件参数为：defaultdict(<function <lambda> at 0x7fd2e93d4af0>, {'pdf': {'two_column': True, 'formula_ocr': True}})\n",
            "2023-06-01 14:04:40,961 [INFO] [base_model.py:437] 回答为：\u001b[34m在 Python 中，元组(group)是由一组元素的列表组成的数据结构。如果你想使用 Python 遍历元组，可以使用以下方法：\n",
            "\n",
            "1. 创建一个元组并遍历：使用 `group_by` 函数和不同的内置函数来遍历元组，其中最常用的是 `group_by` 函数。\n",
            "\n",
            "以下是一个示例代码：\n",
            "\n",
            "```python\n",
            "import redis\n",
            "\n",
            "# 在 Redis 中存储元组数据\n",
            " redis_client = redis.Redis(host='localhost', port=6379)\n",
            "\n",
            "# 创建用于遍历的元组\n",
            "group = {\n",
            "    \"A\": [1, 2, 3],\n",
            "    \"B\": [4, 5, 6],\n",
            "    \"C\": [7, 8, 9]\n",
            "}\n",
            "\n",
            "# 使用 Redis 的 `L孩子学习` 键来获取元组\n",
            "group_by_key = redis_client.l孩子学习(key)\n",
            "\n",
            "# 循环遍历元组\n",
            "for key, value in group_by_key.items():\n",
            "    print(key, value)\n",
            "```\n",
            "\n",
            "在这个例子中，我们首先使用 `group_by` 函数来创建一个包含三个元素的元组，然后使用 Redis `L孩子学习` 键来获取元组。最后，我们使用迭代器来遍历元组并输出每个元素。\n",
            "\n",
            "2. 使用列表方法来遍历元组：有些情况下，你可能需要遍历元组，而不是将其作为元组数组。在这种情况下，你可以使用列表方法来遍历元组。\n",
            "\n",
            "以下是一个示例代码：\n",
            "\n",
            "```python\n",
            "import group_by\n",
            "\n",
            "# 创建一个元组并遍历\n",
            "group = group_by(A=1, B=1)\n",
            "for key, value in group.items():\n",
            "    print(f\"{key} = {value}\")\n",
            "```\n",
            "\n",
            "在这个例子中，我们首先使用 `group_by` 函数来创建一个包含两个元素的元组。然后，我们再次调用 `group_by` 函数，这次将第二个参数设置为非索引值，以便获取元组中元素的顺序顺序。我们使用迭代器来遍历元组并输出每个元素的值。\n",
            "\n",
            "以上两种方法都可以用于 Python 的元组操作，选择这种方法要根据具体情况而定。\u001b[0m\n",
            "2023-06-01 14:05:22,968 [WARNING] [base_model.py:223] billing info not implemented, using default\n",
            "2023-06-01 14:05:24,295 [INFO] [base_model.py:373] 输入为：\u001b[34m微软 Build 大会主要讲了什么？\u001b[0m\n",
            "2023-06-01 14:05:29,142 [INFO] [base_model.py:437] 回答为：\u001b[34m根据提供的搜索结果[[\u001b[0m\n",
            "2023-06-01 14:08:50,790 [WARNING] [base_model.py:223] billing info not implemented, using default\n",
            "2023-06-01 14:08:53,759 [INFO] [base_model.py:373] 输入为：\u001b[34m谷歌 IO 2023 大会主要讲了什么\u001b[0m\n",
            "2023-06-01 14:09:46,766 [INFO] [base_model.py:437] 回答为：\u001b[34m根据提供的搜索结果和相关新闻报道[[1](https://juejin.cn/post/7231752049062248508),[2](https://www.zhihu.com/question/600311875),[3](https://io.google/2023/about/intl/zh/),[4](https://sspai.com/post/79639),[5](https://m.36kr.com/p/2253277466472325),[6](https://finance.sina.cn/tech/2023-05-10/detail-imyruvkz4492167.d.html),[7](https://blog.csdn.net/ ZuoYueLiang/article/details/130619328),[8](http://www.sohu.com/a/669439849_114774),[9](https://www.cnbeta.\u001b[0m\n",
            "2023-06-01 15:03:36,316 [INFO] [models.py:562] 正在加载OpenAI模型: gpt-3.5-turbo\n",
            "2023-06-01 15:03:36,317 [INFO] [models.py:615] Model is set to:  gpt-3.5-turbo\n",
            "2023-06-01 15:03:41,090 [INFO] [models.py:562] 正在加载OpenAI模型: gpt-3.5-turbo\n",
            "2023-06-01 15:03:41,090 [INFO] [models.py:615] Model is set to:  gpt-3.5-turbo\n",
            "2023-06-01 15:03:41,977 [INFO] [models.py:562] 正在加载OpenAI模型: gpt-3.5-turbo\n",
            "2023-06-01 15:03:41,977 [INFO] [models.py:615] Model is set to:  gpt-3.5-turbo\n",
            "2023-06-01 15:03:58,147 [ERROR] [models.py:90] 获取API使用情况失败:API request failed with status code 401: {\n",
            "  \"error\": {\n",
            "    \"message\": \"Invalid authorization header\",\n",
            "    \"type\": \"server_error\",\n",
            "    \"param\": null,\n",
            "    \"code\": null\n",
            "  }\n",
            "}\n",
            "\n",
            "2023-06-01 15:03:59,466 [INFO] [base_model.py:373] 输入为：\u001b[34m请介绍下你自己\u001b[0m\n",
            "JSON parsing error, received content: {\n",
            "JSON parsing error, received content:     \"error\": {\n",
            "JSON parsing error, received content:         \"message\": \"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\",\n",
            "JSON parsing error, received content:         \"type\": \"invalid_request_error\",\n",
            "JSON parsing error, received content:         \"param\": null,\n",
            "JSON parsing error, received content:         \"code\": null\n",
            "JSON parsing error, received content:     }\n",
            "JSON parsing error, received content: }\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ChuanhuChatGPT/modules/models/base_model.py\", line 420, in predict\n",
            "    for chatbot, status_text in iter:\n",
            "  File \"/content/ChuanhuChatGPT/modules/models/base_model.py\", line 249, in stream_next_chatbot\n",
            "    for partial_text in stream_iter:\n",
            "  File \"/content/ChuanhuChatGPT/modules/models/models.py\", line 58, in get_answer_stream_iter\n",
            "    for i in iter:\n",
            "  File \"/content/ChuanhuChatGPT/modules/models/models.py\", line 222, in _decode_chat_response\n",
            "    raise Exception(error_msg)\n",
            "Exception: {    \"error\": {        \"message\": \"You didn't provide an API key. You need to provide your API key in an Authorization header using Bearer auth (i.e. Authorization: Bearer YOUR_KEY), or as the password field (with blank username) if you're accessing the API from your browser and are prompted for a username and password. You can obtain an API key from https://platform.openai.com/account/api-keys.\",        \"type\": \"invalid_request_error\",        \"param\": null,        \"code\": null    }}\n"
          ]
        }
      ],
      "source": [
        "!git pull\n",
        "!python ChuanhuChatbot.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}