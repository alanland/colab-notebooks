{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alanland/colab-notebooks/blob/main/langchain-ChatGLM/langchain-ChatGLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hg8LXAe8mt-G"
      },
      "source": [
        "*斜体文本*# **langchain-ChatGLM**\n",
        "\n",
        "\n",
        "VideoCrafter is an open-source video generation and editing toolbox for crafting video content.\n",
        "\n",
        "More details can be founded in [![GitHub](https://img.shields.io/github/stars/alanland/langchain-ChatGLM?style=social)](https://github.com/alanland/langchain-ChatGLM/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ijIJmBFLebe3",
        "outputId": "d0e7df60-b0fd-4c24-d609-1d485eb79435",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla T4, 15360 MiB, 15101 MiB\n"
          ]
        }
      ],
      "source": [
        "### make sure that CUDA is available in Edit -> Nootbook settings -> GPU\n",
        "!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn_LkaeUmFJQ"
      },
      "source": [
        "## Installnation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4XypmLmfJNw",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title python 3.8 | 不执行则使用系统版本 python\n",
        "!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.8 2  \n",
        "!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.9 1  \n",
        "!python --version  \n",
        "!apt-get update\n",
        "!apt install software-properties-common\n",
        "!sudo dpkg --remove --force-remove-reinstreq python3-pip python3-setuptools python3-wheel\n",
        "!apt-get install python3-pip\n",
        "\n",
        "print('Git clone project and install requirements...')\n",
        "!git clone https://github.com/VideoCrafter/VideoCrafter &> /dev/null\n",
        "%cd VideoCrafter \n",
        "!export PYTHONPATH=/content/VideoCrafter:$PYTHONPATH \n",
        "\n",
        "!python3.8 -m pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!apt update\n",
        "!apt install ffmpeg &> /dev/null  \n",
        "!python3.8 -m pip install pytorch-lightning==1.8.3 omegaconf==2.1.1 einops==0.3.0 transformers==4.25.1\n",
        "!python3.8 -m pip install opencv-python==4.1.2.30 imageio==2.9.0 imageio-ffmpeg==0.4.2\n",
        "!python3.8 -m pip install av moviepy\n",
        "!python3.8 -m pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y8F_z_-hd9w"
      },
      "outputs": [],
      "source": [
        "#@title git clone\n",
        "!git lfs install\n",
        "!git clone https://github.com/alanland/langchain-ChatGLM.git\n",
        "%cd langchain-ChatGLM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title checkout\n",
        "!git checkout main-local\n",
        "!git pull"
      ],
      "metadata": {
        "id": "7udHDXrw4Aq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title install dependencies\n",
        "# 项目中 pdf 加载由先前的 detectron2 替换为使用 paddleocr，如果之前有安装过 detectron2 需要先完成卸载避免引发 tools 冲突\n",
        "#!pip uninstall detectron2\n",
        "\n",
        "# 检查paddleocr依赖，linux环境下paddleocr依赖libX11，libXext\n",
        "#!yum install libX11\n",
        "#!yum install libXext\n",
        "\n",
        "# 安装依赖\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# 验证paddleocr是否成功，首次运行会下载约18M模型到~/.paddleocr\n",
        "#!python loader/image_loader.py"
      ],
      "metadata": {
        "id": "yDxaRuBP4KoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 执行 cli_demo.py 脚本体验命令行交互：\n",
        "!git pull\n",
        "!python cli_demo.py"
      ],
      "metadata": {
        "id": "wpyur0ZG40Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 或执行 webui.py 脚本体验 Web 交互\n",
        "!git pull\n",
        "!python webui.py"
      ],
      "metadata": {
        "id": "vAK6L--U5BIf",
        "outputId": "16236d2c-a7ec-40f5-c478-6ed01d7cedca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "INFO  2023-06-05 10:48:06,662-1d: \n",
            "loading model config\n",
            "llm device: cuda\n",
            "embedding device: cuda\n",
            "dir: /content/langchain-ChatGLM\n",
            "flagging username: cb65bc44d75445af83e42674c08238fa\n",
            "\n",
            "Downloading (…)lve/main/config.json: 100% 838/838 [00:00<00:00, 2.44MB/s]\n",
            "Downloading (…)iguration_chatglm.py: 100% 4.38k/4.38k [00:00<00:00, 19.3MB/s]\n",
            "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b-int4:\n",
            "- configuration_chatglm.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "Loading THUDM/chatglm-6b-int4...\n",
            "Downloading (…)/modeling_chatglm.py: 100% 59.4k/59.4k [00:00<00:00, 282kB/s]\n",
            "Downloading (…)main/quantization.py: 100% 31.0k/31.0k [00:00<00:00, 93.3MB/s]\n",
            "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b-int4:\n",
            "- quantization.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b-int4:\n",
            "- modeling_chatglm.py\n",
            "- quantization.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "Downloading pytorch_model.bin: 100% 3.89G/3.89G [00:22<00:00, 172MB/s]\n",
            "No compiled kernel found.\n",
            "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.c\n",
            "Compiling gcc -O3 -fPIC -std=c99 /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.c -shared -o /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.so\n",
            "Load kernel : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.so\n",
            "Using quantization cache\n",
            "Applying quantization to glm layers\n",
            "Downloading (…)okenizer_config.json: 100% 446/446 [00:00<00:00, 2.25MB/s]\n",
            "Downloading (…)enization_chatglm.py: 100% 17.0k/17.0k [00:00<00:00, 46.7MB/s]\n",
            "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm-6b-int4:\n",
            "- tokenization_chatglm.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "Downloading ice_text.model: 100% 2.71M/2.71M [00:00<00:00, 332MB/s]\n",
            "Loaded the model in 39.00 seconds.\n",
            "Downloading (…)58aa3/.gitattributes: 100% 1.48k/1.48k [00:00<00:00, 6.88MB/s]\n",
            "Downloading (…)026ff58aa3/README.md: 100% 317/317 [00:00<00:00, 1.58MB/s]\n",
            "Downloading (…)6ff58aa3/config.json: 100% 821/821 [00:00<00:00, 4.60MB/s]\n",
            "Downloading (…)aa3/eval_results.txt: 100% 69.0/69.0 [00:00<00:00, 291kB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.30G/1.30G [00:04<00:00, 282MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 125/125 [00:00<00:00, 725kB/s]\n",
            "Downloading (…)58aa3/tokenizer.json: 100% 439k/439k [00:00<00:00, 2.05MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 514/514 [00:00<00:00, 2.95MB/s]\n",
            "Downloading (…)026ff58aa3/vocab.txt: 100% 110k/110k [00:00<00:00, 175MB/s]\n",
            "WARNING 2023-06-05 10:49:01,952-1d: No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/GanymedeNil_text2vec-large-chinese. Creating a new one with MEAN pooling.\n",
            "WARNING 2023-06-05 10:49:05,603-1d: The dtype of attention mask (torch.int64) is not bool\n",
            "2023-06-05 10:49:19.151816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "{'answer': '你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。'}\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://6ddf6f483e0ed65665.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 412, in run_predict\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1299, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1021, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/content/langchain-ChatGLM/webui.py\", line 171, in change_vs_name_input\n",
            "    vs_path = os.path.join(VS_ROOT_PATH, vs_id)\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 90, in join\n",
            "    genericpath._check_arg_types('join', a, *p)\n",
            "  File \"/usr/lib/python3.10/genericpath.py\", line 152, in _check_arg_types\n",
            "    raise TypeError(f'{funcname}() argument must be str, bytes, or '\n",
            "TypeError: join() argument must be str, bytes, or os.PathLike object, not 'NoneType'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 或执行 api.py 利用 fastapi 部署 API\n",
        "!git pull\n",
        "!python api.py\n",
        "%cd views \n",
        "!npm install\n",
        "!npm run dev"
      ],
      "metadata": {
        "id": "C3dsqf1u5DF2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}